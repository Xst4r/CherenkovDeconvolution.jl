{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "For a quick start, we compare the different algorithms for deconvolution on the IRIS data set, estimating the probability density of Iris plant types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the example data\n",
    "using MLDataUtils\n",
    "X, y_labels, _ = load_iris()\n",
    "\n",
    "# discretize the target quantity (for numerical values, we'd use LinearDiscretizer)\n",
    "using Discretizers: encode, CategoricalDiscretizer\n",
    "y = encode(CategoricalDiscretizer(y_labels), y_labels) # vector of target value indices\n",
    "\n",
    "# have a look at the content of y\n",
    "unique(y) # its just indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and observed data sets.\n",
    "# \n",
    "# The matrices MLDataUtils expects are transposed, by default.\n",
    "# Thus, we have to be explicit about obsdim = 1. Note that\n",
    "# CherenkovDeconvolution.jl follows the convention of ScikitLearn.jl\n",
    "# (and others), which is size(X_train) == (n_examples, n_features).\n",
    "# \n",
    "# MLDataUtils unfortunately assumes size(X_train) == (n_features, n_examples),\n",
    "# but obsdim = 1 fixes this assumption.\n",
    "# \n",
    "using Random; Random.seed!(42) # make split reproducible\n",
    "(X_train, y_train), (X_data, y_data) = splitobs(shuffleobs((X', y), obsdim = 1), obsdim = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconvolution with DSEA\n",
    "\n",
    "The Dortmund Spectrum Estimation Algorithm (DSEA) reconstructs the target density from classifier predictions on the target quantity of individual examples. CherenkovDeconvolution.jl implements the improved version DSEA+, which is extended by adaptive step sizes and a fixed reweighting of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: DSEA iteration 1/1 uses alpha = 1.0 (chi2s = 0.0028011676660929232)\n",
      "└ @ CherenkovDeconvolution.Methods /home/bunse/.julia/dev/CherenkovDeconvolution/src/methods/dsea.jl:169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.3333333327844613 \n",
       " 0.3549289670574494 \n",
       " 0.31173770015808927"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn, CherenkovDeconvolution\n",
    "@sk_import naive_bayes : GaussianNB\n",
    "\n",
    "# deconvolve with a Naive Bayes classifier\n",
    "dsea = DSEA(GaussianNB()) # instantiate the deconvolution method\n",
    "f_dsea = deconvolve(dsea, X_data, X_train, y_train) # returns a vector of target value probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.3333333333333333 \n",
       " 0.35555555555555557\n",
       " 0.3111111111111111 "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare the result to the true target distribution, which we are estimating\n",
    "f_true = DeconvUtil.fit_pdf(y_data) # f_dsea is almost equal to f_true!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classical Deconvolution-Algorithms\n",
    "\n",
    "The Regularized Unfolding (RUN) fits the density distribution `f` to the convolution model `g = R * f`, using maximum likelihood. The regularization strength is configured with `n_df`, the effective number of degrees of freedom in the second-order local model of the solution.\n",
    "\n",
    "The Iterative Bayesian Unfolding (IBU) reconstructs the target density by iteratively applying Bayes' rule to the conditional probabilities contained in the detector response matrix.\n",
    "\n",
    "The SVD-based method computes the singular value decomposition of the detector response matrix `R`, fitting `f` according to the method of least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# The classical algorithms are only applicable with a single discrete observable dimension.\n",
    "# In order to obtain a dimension that contains as much information as possible, we discretize\n",
    "# the feature space with a decision tree, using its leaves as clusters. The cluster indices\n",
    "# are the discrete values of the observed dimension.\n",
    "#\n",
    "binning = TreeBinning(6) # obtain (up to) 6 clusters\n",
    "\n",
    "# inspect the way in which the TreeBinning discretizes the data\n",
    "td = BinningDiscretizer(binning, X_train, y_train) # fit the tree with labeled data\n",
    "x_train = encode(td, X_train) # apply it to the feature vectors\n",
    "unique(x_train) # the result are the cluster indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.3333333333333333\n",
       " 0.3463872738499534\n",
       " 0.3202793928167133"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN and IBU need a binning instead of a classifier\n",
    "f_ibu = deconvolve(IBU(binning), X_data, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Limiting RUN to 3 of 6 observeable non-zero bins\n",
      "└ @ CherenkovDeconvolution.Methods /home/bunse/.julia/dev/CherenkovDeconvolution/src/methods/run.jl:110\n",
      "┌ Warning: Performing acceptance correction regularisation requires a given acceptance_correction object\n",
      "└ @ CherenkovDeconvolution.Methods /home/bunse/.julia/dev/CherenkovDeconvolution/src/methods/run.jl:137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.32062181199902845\n",
       " 0.34272528540199176\n",
       " 0.33665290259897984"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_run = deconvolve(RUN(binning), X_data, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Limiting PRUN to 3 of 6 observeable non-zero bins\n",
      "└ @ CherenkovDeconvolution.Methods /home/bunse/.julia/dev/CherenkovDeconvolution/src/methods/prun.jl:113\n",
      "┌ Warning: Performing acceptance correction regularisation requires a given acceptance_correction object\n",
      "└ @ CherenkovDeconvolution.Methods /home/bunse/.julia/dev/CherenkovDeconvolution/src/methods/prun.jl:149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.3206218725187752 \n",
       " 0.34272523890463785\n",
       " 0.3366528885765869 "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_p_run = deconvolve(PRUN(binning), X_data, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.3206218119990284 \n",
       " 0.3427252854019929 \n",
       " 0.33665290259897873"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_svd = deconvolve(SVD(binning), X_data, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mE\u001b[22m\u001b[0m\u001b[1mA\u001b[22m \u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ma\u001b[22m f_\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ma\u001b[22m Gri\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ma\u001b[22mrch \u001b[0m\u001b[1mD\u001b[22men\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mA\u001b[22mrray \u001b[0m\u001b[1mD\u001b[22men\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22mM\u001b[0m\u001b[1ma\u001b[22mtrix \u001b[0m\u001b[1mD\u001b[22men\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22mVecOrM\u001b[0m\u001b[1ma\u001b[22mt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "DSEA(classifier; kwargs...)\n",
       "\\end{verbatim}\n",
       "The \\emph{DSEA/DSEA+} deconvolution method, embedding the given \\texttt{classifier}.\n",
       "\n",
       "\\textbf{Keyword arguments}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{f\\_0 = ones(m) ./ m} defines the prior, which is uniform by default\n",
       "\n",
       "\n",
       "\\item \\texttt{fixweighting = true} sets, whether or not the weight update fix is applied. This fix is proposed in my Master's thesis and in the corresponding paper.\n",
       "\n",
       "\n",
       "\\item \\texttt{stepsize = DEFAULT\\_STEPSIZE} is the step size taken in every iteration.\n",
       "\n",
       "\n",
       "\\item \\texttt{smoothing = Base.identity} is a function that optionally applies smoothing in between iterations.\n",
       "\n",
       "\n",
       "\\item \\texttt{K = 1} is the maximum number of iterations.\n",
       "\n",
       "\n",
       "\\item \\texttt{epsilon = 0.0} is the minimum symmetric Chi Square distance between iterations. If the actual distance is below this threshold, convergence is assumed and the algorithm stops.\n",
       "\n",
       "\n",
       "\\item \\texttt{inspect = nothing} is a function \\texttt{(f\\_k::Vector, k::Int, chi2s::Float64, alpha\\_k::Float64) -> Any} optionally called in every iteration.\n",
       "\n",
       "\n",
       "\\item \\texttt{return\\_contributions = false} sets, whether or not the contributions of individual examples in \\texttt{X\\_data} are returned as a tuple together with the deconvolution result.\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "DSEA(classifier; kwargs...)\n",
       "```\n",
       "\n",
       "The *DSEA/DSEA+* deconvolution method, embedding the given `classifier`.\n",
       "\n",
       "**Keyword arguments**\n",
       "\n",
       "  * `f_0 = ones(m) ./ m` defines the prior, which is uniform by default\n",
       "  * `fixweighting = true` sets, whether or not the weight update fix is applied. This fix is proposed in my Master's thesis and in the corresponding paper.\n",
       "  * `stepsize = DEFAULT_STEPSIZE` is the step size taken in every iteration.\n",
       "  * `smoothing = Base.identity` is a function that optionally applies smoothing in between iterations.\n",
       "  * `K = 1` is the maximum number of iterations.\n",
       "  * `epsilon = 0.0` is the minimum symmetric Chi Square distance between iterations. If the actual distance is below this threshold, convergence is assumed and the algorithm stops.\n",
       "  * `inspect = nothing` is a function `(f_k::Vector, k::Int, chi2s::Float64, alpha_k::Float64) -> Any` optionally called in every iteration.\n",
       "  * `return_contributions = false` sets, whether or not the contributions of individual examples in `X_data` are returned as a tuple together with the deconvolution result.\n"
      ],
      "text/plain": [
       "\u001b[36m  DSEA(classifier; kwargs...)\u001b[39m\n",
       "\n",
       "  The \u001b[4mDSEA/DSEA+\u001b[24m deconvolution method, embedding the given \u001b[36mclassifier\u001b[39m.\n",
       "\n",
       "  \u001b[1mKeyword arguments\u001b[22m\n",
       "\n",
       "    •    \u001b[36mf_0 = ones(m) ./ m\u001b[39m defines the prior, which is uniform by default\n",
       "\n",
       "    •    \u001b[36mfixweighting = true\u001b[39m sets, whether or not the weight update fix is\n",
       "        applied. This fix is proposed in my Master's thesis and in the\n",
       "        corresponding paper.\n",
       "\n",
       "    •    \u001b[36mstepsize = DEFAULT_STEPSIZE\u001b[39m is the step size taken in every\n",
       "        iteration.\n",
       "\n",
       "    •    \u001b[36msmoothing = Base.identity\u001b[39m is a function that optionally applies\n",
       "        smoothing in between iterations.\n",
       "\n",
       "    •    \u001b[36mK = 1\u001b[39m is the maximum number of iterations.\n",
       "\n",
       "    •    \u001b[36mepsilon = 0.0\u001b[39m is the minimum symmetric Chi Square distance between\n",
       "        iterations. If the actual distance is below this threshold,\n",
       "        convergence is assumed and the algorithm stops.\n",
       "\n",
       "    •    \u001b[36minspect = nothing\u001b[39m is a function \u001b[36m(f_k::Vector, k::Int,\n",
       "        chi2s::Float64, alpha_k::Float64) -> Any\u001b[39m optionally called in\n",
       "        every iteration.\n",
       "\n",
       "    •    \u001b[36mreturn_contributions = false\u001b[39m sets, whether or not the\n",
       "        contributions of individual examples in \u001b[36mX_data\u001b[39m are returned as a\n",
       "        tuple together with the deconvolution result."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?DSEA # You can find more information in the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mI\u001b[22m\u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1mU\u001b[22m f_\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mb\u001b[22m\u001b[0m\u001b[1mu\u001b[22m \u001b[0m\u001b[1mI\u001b[22mO\u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1mu\u001b[22mffer @\u001b[0m\u001b[1mi\u001b[22mn\u001b[0m\u001b[1mb\u001b[22mo\u001b[0m\u001b[1mu\u001b[22mnds P\u001b[0m\u001b[1mi\u001b[22mpe\u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1mu\u001b[22mffer\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "IBU(binning; kwargs...)\n",
       "\\end{verbatim}\n",
       "The \\emph{Iterative Bayesian Unfolding} deconvolution method, using a \\texttt{binning} to discretize the observable features.\n",
       "\n",
       "\\textbf{Keyword arguments}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{f\\_0 = ones(m) ./ m} defines the prior, which is uniform by default.\n",
       "\n",
       "\n",
       "\\item \\texttt{smoothing = Base.identity} is a function that optionally applies smoothing in between iterations. The operation is neither applied to the initial prior, nor to the final result. The function \\texttt{inspect} is called before the smoothing is performed.\n",
       "\n",
       "\n",
       "\\item \\texttt{K = 3} is the maximum number of iterations.\n",
       "\n",
       "\n",
       "\\item \\texttt{epsilon = 0.0} is the minimum symmetric Chi Square distance between iterations. If the actual distance is below this threshold, convergence is assumed and the algorithm stops.\n",
       "\n",
       "\n",
       "\\item \\texttt{stepsize = DEFAULT\\_STEPSIZE} is the step size taken in every iteration.\n",
       "\n",
       "\n",
       "\\item \\texttt{fit\\_ratios = false} determines if ratios are fitted (i.e. \\texttt{R} has to contain counts so that the ratio \\texttt{f\\_est / f\\_train} is estimated) or if the probability density \\texttt{f\\_est} is fitted directly.\n",
       "\n",
       "\n",
       "\\item \\texttt{inspect = nothing} is a function \\texttt{(f\\_k::Vector, k::Int, chi2s::Float64, alpha\\_k::Float64) -> Any} optionally called in every iteration.\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "IBU(binning; kwargs...)\n",
       "```\n",
       "\n",
       "The *Iterative Bayesian Unfolding* deconvolution method, using a `binning` to discretize the observable features.\n",
       "\n",
       "**Keyword arguments**\n",
       "\n",
       "  * `f_0 = ones(m) ./ m` defines the prior, which is uniform by default.\n",
       "  * `smoothing = Base.identity` is a function that optionally applies smoothing in between iterations. The operation is neither applied to the initial prior, nor to the final result. The function `inspect` is called before the smoothing is performed.\n",
       "  * `K = 3` is the maximum number of iterations.\n",
       "  * `epsilon = 0.0` is the minimum symmetric Chi Square distance between iterations. If the actual distance is below this threshold, convergence is assumed and the algorithm stops.\n",
       "  * `stepsize = DEFAULT_STEPSIZE` is the step size taken in every iteration.\n",
       "  * `fit_ratios = false` determines if ratios are fitted (i.e. `R` has to contain counts so that the ratio `f_est / f_train` is estimated) or if the probability density `f_est` is fitted directly.\n",
       "  * `inspect = nothing` is a function `(f_k::Vector, k::Int, chi2s::Float64, alpha_k::Float64) -> Any` optionally called in every iteration.\n"
      ],
      "text/plain": [
       "\u001b[36m  IBU(binning; kwargs...)\u001b[39m\n",
       "\n",
       "  The \u001b[4mIterative Bayesian Unfolding\u001b[24m deconvolution method, using a \u001b[36mbinning\u001b[39m to\n",
       "  discretize the observable features.\n",
       "\n",
       "  \u001b[1mKeyword arguments\u001b[22m\n",
       "\n",
       "    •    \u001b[36mf_0 = ones(m) ./ m\u001b[39m defines the prior, which is uniform by default.\n",
       "\n",
       "    •    \u001b[36msmoothing = Base.identity\u001b[39m is a function that optionally applies\n",
       "        smoothing in between iterations. The operation is neither applied\n",
       "        to the initial prior, nor to the final result. The function\n",
       "        \u001b[36minspect\u001b[39m is called before the smoothing is performed.\n",
       "\n",
       "    •    \u001b[36mK = 3\u001b[39m is the maximum number of iterations.\n",
       "\n",
       "    •    \u001b[36mepsilon = 0.0\u001b[39m is the minimum symmetric Chi Square distance between\n",
       "        iterations. If the actual distance is below this threshold,\n",
       "        convergence is assumed and the algorithm stops.\n",
       "\n",
       "    •    \u001b[36mstepsize = DEFAULT_STEPSIZE\u001b[39m is the step size taken in every\n",
       "        iteration.\n",
       "\n",
       "    •    \u001b[36mfit_ratios = false\u001b[39m determines if ratios are fitted (i.e. \u001b[36mR\u001b[39m has to\n",
       "        contain counts so that the ratio \u001b[36mf_est / f_train\u001b[39m is estimated) or\n",
       "        if the probability density \u001b[36mf_est\u001b[39m is fitted directly.\n",
       "\n",
       "    •    \u001b[36minspect = nothing\u001b[39m is a function \u001b[36m(f_k::Vector, k::Int,\n",
       "        chi2s::Float64, alpha_k::Float64) -> Any\u001b[39m optionally called in\n",
       "        every iteration."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?IBU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mR\u001b[22m\u001b[0m\u001b[1mU\u001b[22m\u001b[0m\u001b[1mN\u001b[22m \u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mn\u001b[22m \u001b[0m\u001b[1mR\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mn\u001b[22mStepsize P\u001b[0m\u001b[1mR\u001b[22m\u001b[0m\u001b[1mU\u001b[22m\u001b[0m\u001b[1mN\u001b[22m t\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mn\u001b[22mc t\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mn\u001b[22mcate f_\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mn\u001b[22m \u001b[0m\u001b[1mr\u001b[22mo\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mn\u001b[22md \u001b[0m\u001b[1mR\u001b[22mo\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mn\u001b[22mdUp f_p_\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "RUN(binning; kwargs...)\n",
       "\\end{verbatim}\n",
       "The \\emph{Regularized Unfolding} method, using a \\texttt{binning} to discretize the observable features.\n",
       "\n",
       "\\textbf{Keyword arguments}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{n\\_df = size(R, 2)} is the effective number of degrees of freedom. The default \\texttt{n\\_df} results in no regularization (there is one degree of freedom for each dimension in the result).\n",
       "\n",
       "\n",
       "\\item \\texttt{K = 100} is the maximum number of iterations.\n",
       "\n",
       "\n",
       "\\item \\texttt{epsilon = 1e-6} is the minimum difference in the loss function between iterations. RUN stops when the absolute loss difference drops below \\texttt{epsilon}.\n",
       "\n",
       "\n",
       "\\item \\texttt{acceptance\\_correction = nothing}  is a tuple of functions (ac(d), inv\\emph{ac(d)) representing the acceptance correction ac and its inverse operation inv}ac for a data set d.\n",
       "\n",
       "\n",
       "\\item \\texttt{ac\\_regularisation = true}  decides whether acceptance correction is taken into account for regularisation. Requires \\texttt{acceptance\\_correction} != nothing.\n",
       "\n",
       "\n",
       "\\item \\texttt{log\\_constant = 1/18394} is a selectable constant used in log regularisation to prevent the undefined case log(0).\n",
       "\n",
       "\n",
       "\\item \\texttt{inspect = nothing} is a function \\texttt{(f\\_k::Vector, k::Int, ldiff::Float64, tau::Float64) -> Any} optionally called in every iteration.\n",
       "\n",
       "\n",
       "\\item \\texttt{fit\\_ratios = false} determines if ratios are fitted (i.e. \\texttt{R} has to contain counts so that the ratio \\texttt{f\\_est / f\\_train} is estimated) or if the probability density \\texttt{f\\_est} is fitted directly.\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "RUN(binning; kwargs...)\n",
       "```\n",
       "\n",
       "The *Regularized Unfolding* method, using a `binning` to discretize the observable features.\n",
       "\n",
       "**Keyword arguments**\n",
       "\n",
       "  * `n_df = size(R, 2)` is the effective number of degrees of freedom. The default `n_df` results in no regularization (there is one degree of freedom for each dimension in the result).\n",
       "  * `K = 100` is the maximum number of iterations.\n",
       "  * `epsilon = 1e-6` is the minimum difference in the loss function between iterations. RUN stops when the absolute loss difference drops below `epsilon`.\n",
       "  * `acceptance_correction = nothing`  is a tuple of functions (ac(d), inv*ac(d)) representing the acceptance correction ac and its inverse operation inv*ac for a data set d.\n",
       "  * `ac_regularisation = true`  decides whether acceptance correction is taken into account for regularisation. Requires `acceptance_correction` != nothing.\n",
       "  * `log_constant = 1/18394` is a selectable constant used in log regularisation to prevent the undefined case log(0).\n",
       "  * `inspect = nothing` is a function `(f_k::Vector, k::Int, ldiff::Float64, tau::Float64) -> Any` optionally called in every iteration.\n",
       "  * `fit_ratios = false` determines if ratios are fitted (i.e. `R` has to contain counts so that the ratio `f_est / f_train` is estimated) or if the probability density `f_est` is fitted directly.\n"
      ],
      "text/plain": [
       "\u001b[36m  RUN(binning; kwargs...)\u001b[39m\n",
       "\n",
       "  The \u001b[4mRegularized Unfolding\u001b[24m method, using a \u001b[36mbinning\u001b[39m to discretize the\n",
       "  observable features.\n",
       "\n",
       "  \u001b[1mKeyword arguments\u001b[22m\n",
       "\n",
       "    •    \u001b[36mn_df = size(R, 2)\u001b[39m is the effective number of degrees of freedom.\n",
       "        The default \u001b[36mn_df\u001b[39m results in no regularization (there is one degree\n",
       "        of freedom for each dimension in the result).\n",
       "\n",
       "    •    \u001b[36mK = 100\u001b[39m is the maximum number of iterations.\n",
       "\n",
       "    •    \u001b[36mepsilon = 1e-6\u001b[39m is the minimum difference in the loss function\n",
       "        between iterations. RUN stops when the absolute loss difference\n",
       "        drops below \u001b[36mepsilon\u001b[39m.\n",
       "\n",
       "    •    \u001b[36macceptance_correction = nothing\u001b[39m is a tuple of functions (ac(d),\n",
       "        inv\u001b[4mac(d)) representing the acceptance correction ac and its\n",
       "        inverse operation inv\u001b[24mac for a data set d.\n",
       "\n",
       "    •    \u001b[36mac_regularisation = true\u001b[39m decides whether acceptance correction is\n",
       "        taken into account for regularisation. Requires\n",
       "        \u001b[36macceptance_correction\u001b[39m != nothing.\n",
       "\n",
       "    •    \u001b[36mlog_constant = 1/18394\u001b[39m is a selectable constant used in log\n",
       "        regularisation to prevent the undefined case log(0).\n",
       "\n",
       "    •    \u001b[36minspect = nothing\u001b[39m is a function \u001b[36m(f_k::Vector, k::Int,\n",
       "        ldiff::Float64, tau::Float64) -> Any\u001b[39m optionally called in every\n",
       "        iteration.\n",
       "\n",
       "    •    \u001b[36mfit_ratios = false\u001b[39m determines if ratios are fitted (i.e. \u001b[36mR\u001b[39m has to\n",
       "        contain counts so that the ratio \u001b[36mf_est / f_train\u001b[39m is estimated) or\n",
       "        if the probability density \u001b[36mf_est\u001b[39m is fitted directly."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mP\u001b[22m\u001b[0m\u001b[1mR\u001b[22m\u001b[0m\u001b[1mU\u001b[22m\u001b[0m\u001b[1mN\u001b[22m f_\u001b[0m\u001b[1mp\u001b[22m_\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mn\u001b[22m \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mr\u001b[22mocess_r\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mn\u001b[22mning al\u001b[0m\u001b[1mp\u001b[22mha_adaptive_\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mn\u001b[22m \u001b[0m\u001b[1mp\u001b[22me\u001b[0m\u001b[1mr\u001b[22mm\u001b[0m\u001b[1mu\u001b[22mte! \u001b[0m\u001b[1mp\u001b[22me\u001b[0m\u001b[1mr\u001b[22mm\u001b[0m\u001b[1mu\u001b[22mtedims\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "PRUN(binning; kwargs...)\n",
       "\\end{verbatim}\n",
       "A version of the \\emph{Regularized Unfolding} method that is constrained to positive results. Like the original version, it uses a \\texttt{binning} to discretize the observable features.\n",
       "\n",
       "\\textbf{Keyword arguments}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{tau = 0.0} determines the regularisation strength.\n",
       "\n",
       "\n",
       "\\item \\texttt{K = 100} is the maximum number of iterations.\n",
       "\n",
       "\n",
       "\\item \\texttt{epsilon = 1e-6} is the minimum difference in the loss function between iterations. RUN stops when the absolute loss difference drops below \\texttt{epsilon}.\n",
       "\n",
       "\n",
       "\\item \\texttt{f\\_0 = ones(size(R, 2))} Starting point for the interior-point Newton optimization.\n",
       "\n",
       "\n",
       "\\item \\texttt{acceptance\\_correction = nothing}  is a tuple of functions (ac(d), inv\\emph{ac(d)) representing the acceptance correction ac and its inverse operation inv}ac for a data set d.\n",
       "\n",
       "\n",
       "\\item \\texttt{ac\\_regularisation = true}  decides whether acceptance correction is taken into account for regularisation. Requires \\texttt{acceptance\\_correction} != nothing.\n",
       "\n",
       "\n",
       "\\item \\texttt{log\\_constant = 1/18394} is a selectable constant used in log regularisation to prevent the undefined case log(0).\n",
       "\n",
       "\n",
       "\\item \\texttt{inspect = nothing} is a function \\texttt{(f\\_k::Vector, k::Int, ldiff::Float64) -> Any} called in each iteration.\n",
       "\n",
       "\n",
       "\\item \\texttt{fit\\_ratios = false} determines if ratios are fitted (i.e. \\texttt{R} has to contain counts so that the ratio \\texttt{f\\_est / f\\_train} is estimated) or if the probability density \\texttt{f\\_est} is fitted directly.\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "PRUN(binning; kwargs...)\n",
       "```\n",
       "\n",
       "A version of the *Regularized Unfolding* method that is constrained to positive results. Like the original version, it uses a `binning` to discretize the observable features.\n",
       "\n",
       "**Keyword arguments**\n",
       "\n",
       "  * `tau = 0.0` determines the regularisation strength.\n",
       "  * `K = 100` is the maximum number of iterations.\n",
       "  * `epsilon = 1e-6` is the minimum difference in the loss function between iterations. RUN stops when the absolute loss difference drops below `epsilon`.\n",
       "  * `f_0 = ones(size(R, 2))` Starting point for the interior-point Newton optimization.\n",
       "  * `acceptance_correction = nothing`  is a tuple of functions (ac(d), inv*ac(d)) representing the acceptance correction ac and its inverse operation inv*ac for a data set d.\n",
       "  * `ac_regularisation = true`  decides whether acceptance correction is taken into account for regularisation. Requires `acceptance_correction` != nothing.\n",
       "  * `log_constant = 1/18394` is a selectable constant used in log regularisation to prevent the undefined case log(0).\n",
       "  * `inspect = nothing` is a function `(f_k::Vector, k::Int, ldiff::Float64) -> Any` called in each iteration.\n",
       "  * `fit_ratios = false` determines if ratios are fitted (i.e. `R` has to contain counts so that the ratio `f_est / f_train` is estimated) or if the probability density `f_est` is fitted directly.\n"
      ],
      "text/plain": [
       "\u001b[36m  PRUN(binning; kwargs...)\u001b[39m\n",
       "\n",
       "  A version of the \u001b[4mRegularized Unfolding\u001b[24m method that is constrained to\n",
       "  positive results. Like the original version, it uses a \u001b[36mbinning\u001b[39m to discretize\n",
       "  the observable features.\n",
       "\n",
       "  \u001b[1mKeyword arguments\u001b[22m\n",
       "\n",
       "    •    \u001b[36mtau = 0.0\u001b[39m determines the regularisation strength.\n",
       "\n",
       "    •    \u001b[36mK = 100\u001b[39m is the maximum number of iterations.\n",
       "\n",
       "    •    \u001b[36mepsilon = 1e-6\u001b[39m is the minimum difference in the loss function\n",
       "        between iterations. RUN stops when the absolute loss difference\n",
       "        drops below \u001b[36mepsilon\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mf_0 = ones(size(R, 2))\u001b[39m Starting point for the interior-point\n",
       "        Newton optimization.\n",
       "\n",
       "    •    \u001b[36macceptance_correction = nothing\u001b[39m is a tuple of functions (ac(d),\n",
       "        inv\u001b[4mac(d)) representing the acceptance correction ac and its\n",
       "        inverse operation inv\u001b[24mac for a data set d.\n",
       "\n",
       "    •    \u001b[36mac_regularisation = true\u001b[39m decides whether acceptance correction is\n",
       "        taken into account for regularisation. Requires\n",
       "        \u001b[36macceptance_correction\u001b[39m != nothing.\n",
       "\n",
       "    •    \u001b[36mlog_constant = 1/18394\u001b[39m is a selectable constant used in log\n",
       "        regularisation to prevent the undefined case log(0).\n",
       "\n",
       "    •    \u001b[36minspect = nothing\u001b[39m is a function \u001b[36m(f_k::Vector, k::Int,\n",
       "        ldiff::Float64) -> Any\u001b[39m called in each iteration.\n",
       "\n",
       "    •    \u001b[36mfit_ratios = false\u001b[39m determines if ratios are fitted (i.e. \u001b[36mR\u001b[39m has to\n",
       "        contain counts so that the ratio \u001b[36mf_est / f_train\u001b[39m is estimated) or\n",
       "        if the probability density \u001b[36mf_est\u001b[39m is fitted directly."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?PRUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mV\u001b[22m\u001b[0m\u001b[1mD\u001b[22m f_\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mv\u001b[22m\u001b[0m\u001b[1md\u001b[22m i\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mv\u001b[22mali\u001b[0m\u001b[1md\u001b[22m Cro\u001b[0m\u001b[1ms\u001b[22ms\u001b[0m\u001b[1mV\u001b[22mali\u001b[0m\u001b[1md\u001b[22mation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "SVD(binning; kwargs...)\n",
       "\\end{verbatim}\n",
       "The \\emph{SVD-based} deconvolution method, using a \\texttt{binning} to discretize the observable features.\n",
       "\n",
       "\\textbf{Keyword arguments}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{effective\\_rank = -1} is a regularization parameter which defines the effective rank of the solution. This rank must be <= dim(f). Any value smaller than one results turns off regularization.\n",
       "\n",
       "\n",
       "\\item \\texttt{N = sum(g)} is the number of observations.\n",
       "\n",
       "\n",
       "\\item \\texttt{B = DeconvUtil.cov\\_Poisson(g, N)} is the varianca-covariance matrix of the observed bins. The default value represents the assumption that each observed bin is Poisson-distributed with rate \\texttt{g[i]*N}.\n",
       "\n",
       "\n",
       "\\item \\texttt{epsilon\\_C = 1e-3} is a small constant to be added to each diagonal entry of the regularization matrix \\texttt{C}. If no such constant would be added, inversion of \\texttt{C} would not be possible.\n",
       "\n",
       "\n",
       "\\item \\texttt{fit\\_ratios = false} determines if ratios are fitted (i.e. \\texttt{R} has to contain counts so that the ratio \\texttt{f\\_est / f\\_train} is estimated) or if the probability density \\texttt{f\\_est} is fitted directly.\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "SVD(binning; kwargs...)\n",
       "```\n",
       "\n",
       "The *SVD-based* deconvolution method, using a `binning` to discretize the observable features.\n",
       "\n",
       "**Keyword arguments**\n",
       "\n",
       "  * `effective_rank = -1` is a regularization parameter which defines the effective rank of the solution. This rank must be <= dim(f). Any value smaller than one results turns off regularization.\n",
       "  * `N = sum(g)` is the number of observations.\n",
       "  * `B = DeconvUtil.cov_Poisson(g, N)` is the varianca-covariance matrix of the observed bins. The default value represents the assumption that each observed bin is Poisson-distributed with rate `g[i]*N`.\n",
       "  * `epsilon_C = 1e-3` is a small constant to be added to each diagonal entry of the regularization matrix `C`. If no such constant would be added, inversion of `C` would not be possible.\n",
       "  * `fit_ratios = false` determines if ratios are fitted (i.e. `R` has to contain counts so that the ratio `f_est / f_train` is estimated) or if the probability density `f_est` is fitted directly.\n"
      ],
      "text/plain": [
       "\u001b[36m  SVD(binning; kwargs...)\u001b[39m\n",
       "\n",
       "  The \u001b[4mSVD-based\u001b[24m deconvolution method, using a \u001b[36mbinning\u001b[39m to discretize the\n",
       "  observable features.\n",
       "\n",
       "  \u001b[1mKeyword arguments\u001b[22m\n",
       "\n",
       "    •    \u001b[36meffective_rank = -1\u001b[39m is a regularization parameter which defines\n",
       "        the effective rank of the solution. This rank must be <= dim(f).\n",
       "        Any value smaller than one results turns off regularization.\n",
       "\n",
       "    •    \u001b[36mN = sum(g)\u001b[39m is the number of observations.\n",
       "\n",
       "    •    \u001b[36mB = DeconvUtil.cov_Poisson(g, N)\u001b[39m is the varianca-covariance matrix\n",
       "        of the observed bins. The default value represents the assumption\n",
       "        that each observed bin is Poisson-distributed with rate \u001b[36mg[i]*N\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mepsilon_C = 1e-3\u001b[39m is a small constant to be added to each diagonal\n",
       "        entry of the regularization matrix \u001b[36mC\u001b[39m. If no such constant would be\n",
       "        added, inversion of \u001b[36mC\u001b[39m would not be possible.\n",
       "\n",
       "    •    \u001b[36mfit_ratios = false\u001b[39m determines if ratios are fitted (i.e. \u001b[36mR\u001b[39m has to\n",
       "        contain counts so that the ratio \u001b[36mf_est / f_train\u001b[39m is estimated) or\n",
       "        if the probability density \u001b[36mf_est\u001b[39m is fitted directly."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?SVD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
