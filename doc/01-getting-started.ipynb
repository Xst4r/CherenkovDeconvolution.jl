{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "For a quick start, we compare the different algorithms for deconvolution on the IRIS data set, estimating the probability density of Iris plant types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the example data\n",
    "using MLDataUtils\n",
    "X, y_labels, _ = load_iris()\n",
    "\n",
    "# discretize the target quantity (for numerical values, we'd use LinearDiscretizer)\n",
    "using Discretizers: encode, CategoricalDiscretizer\n",
    "y = encode(CategoricalDiscretizer(y_labels), y_labels) # vector of target value indices\n",
    "\n",
    "# have a look at the content of y\n",
    "unique(y) # its just indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and observed data sets.\n",
    "# \n",
    "# The matrices MLDataUtils expects are transposed, by default.\n",
    "# Thus, we have to be explicit about obsdim = 1. Note that\n",
    "# CherenkovDeconvolution.jl follows the convention of ScikitLearn.jl\n",
    "# (and others), which is size(X_train) == (n_examples, n_features).\n",
    "# \n",
    "# MLDataUtils unfortunately assumes size(X_train) == (n_features, n_examples),\n",
    "# but obsdim = 1 fixes this assumption.\n",
    "# \n",
    "using Random; Random.seed!(42) # make split reproducible\n",
    "(X_train, y_train), (X_data, y_data) = splitobs(shuffleobs((X', y), obsdim = 1), obsdim = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconvolution with DSEA\n",
    "\n",
    "The Dortmund Spectrum Estimation Algorithm (DSEA) reconstructs the target density from classifier predictions on the target quantity of individual examples. CherenkovDeconvolution.jl implements the improved version DSEA+, which is extended by adaptive step sizes and a fixed reweighting of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: DSEA iteration 1/1 uses alpha = 1.0 (chi2s = 0.0028011676660929232)\n",
      "└ @ CherenkovDeconvolution /home/bunse/.julia/dev/CherenkovDeconvolution/src/methods/dsea.jl:150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.3333333327844613 \n",
       " 0.3549289670574494 \n",
       " 0.31173770015808927"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn, CherenkovDeconvolution\n",
    "\n",
    "# deconvolve with a Naive Bayes classifier\n",
    "@sk_import naive_bayes : GaussianNB\n",
    "tp = DeconvUtil.train_and_predict_proba(GaussianNB()) # will train and apply the classifier in each iteration\n",
    "\n",
    "f_dsea = dsea(X_data, X_train, y_train, tp) # returns a vector of target value probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.3333333333333333 \n",
       " 0.35555555555555557\n",
       " 0.3111111111111111 "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare the result to the true target distribution, which we are estimating\n",
    "f_true = DeconvUtil.fit_pdf(y_data) # f_dsea is almost equal to f_true!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classical Deconvolution-Algorithms\n",
    "\n",
    "The Regularized Unfolding (RUN) fits the density distribution `f` to the convolution model `g = R * f`, using maximum likelihood. The regularization strength is configured with `n_df`, the effective number of degrees of freedom in the second-order local model of the solution.\n",
    "\n",
    "The Iterative Bayesian Unfolding (IBU) reconstructs the target density by iteratively applying Bayes' rule to the conditional probabilities contained in the detector response matrix.\n",
    "\n",
    "The SVD-based method computes the singular value decomposition of the detector response matrix `R`, fitting `f` according to the method of least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# The classical algorithms are only applicable with a single discrete observable dimension.\n",
    "# In order to obtain a dimension that contains as much information as possible, we discretize\n",
    "# the feature space with a decision tree, using its leaves as clusters. The cluster indices\n",
    "# are the discrete values of the observed dimension.\n",
    "#\n",
    "td = TreeDiscretizer(X_train, y_train, 6) # obtain (up to) 6 clusters\n",
    "x_train = encode(td, X_train)\n",
    "x_data  = encode(td, X_data)\n",
    "\n",
    "# have a look at the content of x_train\n",
    "unique(x_train) # its the cluster indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Limiting RUN to 3 of 6 observeable non-zero bins\n",
      "└ @ CherenkovDeconvolution /home/bunse/.julia/dev/CherenkovDeconvolution/src/methods/run.jl:103\n",
      "┌ Warning: Performing acceptance correction regularisation requires a given acceptance_correction object\n",
      "└ @ CherenkovDeconvolution /home/bunse/.julia/dev/CherenkovDeconvolution/src/methods/run.jl:133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.32062181199902845\n",
       " 0.34272528540199176\n",
       " 0.33665290259897984"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# However, RUN and IBU do not need a classifier for deconvolution\n",
    "f_run = CherenkovDeconvolution.run(x_data, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Limiting p_RUN to 3 of 6 observeable non-zero bins\n",
      "└ @ CherenkovDeconvolution /home/bunse/.julia/dev/CherenkovDeconvolution/src/methods/p_run.jl:105\n",
      "┌ Warning: Performing acceptance correction regularisation requires a given acceptance_correction object\n",
      "└ @ CherenkovDeconvolution /home/bunse/.julia/dev/CherenkovDeconvolution/src/methods/p_run.jl:129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.32062185296050366\n",
       " 0.34272525419868627\n",
       " 0.3366528928408101 "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_p_run = CherenkovDeconvolution.p_run(x_data, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.3333333333333333\n",
       " 0.3463872738499534\n",
       " 0.3202793928167133"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_ibu = CherenkovDeconvolution.ibu(x_data, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.3206218119990271 \n",
       " 0.34272528540199193\n",
       " 0.33665290259898095"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_svd = CherenkovDeconvolution.svd(x_data, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ma\u001b[22m f_\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ma\u001b[22m Gri\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ma\u001b[22mrch \u001b[0m\u001b[1mD\u001b[22men\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mA\u001b[22mrray \u001b[0m\u001b[1mD\u001b[22men\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22mM\u001b[0m\u001b[1ma\u001b[22mtrix \u001b[0m\u001b[1mD\u001b[22men\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22mVecOrM\u001b[0m\u001b[1ma\u001b[22mt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "dsea(data, train, y, train_predict[, bins_y, features]; kwargs...)\n",
       "\n",
       "dsea(X_data, X_train, y_train, train_predict[, bins_y]; kwargs...)\n",
       "\\end{verbatim}\n",
       "Deconvolve the observed data with \\emph{DSEA/DSEA+} trained on the given training set.\n",
       "\n",
       "The data is provided as feature matrices \\texttt{X\\_data}, \\texttt{X\\_train} and the label vector \\texttt{y\\_train} (or accordingly \\texttt{data[features]}, \\texttt{train[features]}, and \\texttt{train[y]}). Here, \\texttt{y\\_train} must contain label indices rather than actual values. All expected indices are optionally provided as \\texttt{bins\\_y}.\n",
       "\n",
       "The function object \\texttt{train\\_predict(X\\_data, X\\_train, y\\_train, w\\_train) -> Matrix} trains and applies a classifier to obtain a confidence matrix.\n",
       "\n",
       "\\textbf{Keyword arguments}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{f\\_0 = ones(m) ./ m} defines the prior, which is uniform by default\n",
       "\n",
       "\n",
       "\\item \\texttt{fixweighting = true} sets, whether or not the weight update fix is applied. This fix is proposed in my Master's thesis and in the corresponding paper.\n",
       "\n",
       "\n",
       "\\item \\texttt{alpha = DEFAULT\\_STEPSIZE} is the step size taken in every iteration.\n",
       "\n",
       "\n",
       "\\item \\texttt{smoothing = Base.identity} is a function that optionally applies smoothing in between iterations.\n",
       "\n",
       "\n",
       "\\item \\texttt{K = 1} is the maximum number of iterations.\n",
       "\n",
       "\n",
       "\\item \\texttt{epsilon = 0.0} is the minimum symmetric Chi Square distance between iterations. If the actual distance is below this threshold, convergence is assumed and the algorithm stops.\n",
       "\n",
       "\n",
       "\\item \\texttt{inspect = nothing} is a function \\texttt{(f\\_k::Vector, k::Int, chi2s::Float64, alphak::Float64) -> Any} optionally called in every iteration.\n",
       "\n",
       "\n",
       "\\item \\texttt{return\\_contributions = false} sets, whether or not the contributions of individual examples in \\texttt{X\\_data} are returned as a tuple together with the deconvolution result.\n",
       "\n",
       "\n",
       "\\item \\texttt{features = setdiff(names(train), [y])} specifies which columns in \\texttt{data} and \\texttt{train} to be used as features - only applicable to the first form of this function.\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "dsea(data, train, y, train_predict[, bins_y, features]; kwargs...)\n",
       "\n",
       "dsea(X_data, X_train, y_train, train_predict[, bins_y]; kwargs...)\n",
       "```\n",
       "\n",
       "Deconvolve the observed data with *DSEA/DSEA+* trained on the given training set.\n",
       "\n",
       "The data is provided as feature matrices `X_data`, `X_train` and the label vector `y_train` (or accordingly `data[features]`, `train[features]`, and `train[y]`). Here, `y_train` must contain label indices rather than actual values. All expected indices are optionally provided as `bins_y`.\n",
       "\n",
       "The function object `train_predict(X_data, X_train, y_train, w_train) -> Matrix` trains and applies a classifier to obtain a confidence matrix.\n",
       "\n",
       "**Keyword arguments**\n",
       "\n",
       "  * `f_0 = ones(m) ./ m` defines the prior, which is uniform by default\n",
       "  * `fixweighting = true` sets, whether or not the weight update fix is applied. This fix is proposed in my Master's thesis and in the corresponding paper.\n",
       "  * `alpha = DEFAULT_STEPSIZE` is the step size taken in every iteration.\n",
       "  * `smoothing = Base.identity` is a function that optionally applies smoothing in between iterations.\n",
       "  * `K = 1` is the maximum number of iterations.\n",
       "  * `epsilon = 0.0` is the minimum symmetric Chi Square distance between iterations. If the actual distance is below this threshold, convergence is assumed and the algorithm stops.\n",
       "  * `inspect = nothing` is a function `(f_k::Vector, k::Int, chi2s::Float64, alphak::Float64) -> Any` optionally called in every iteration.\n",
       "  * `return_contributions = false` sets, whether or not the contributions of individual examples in `X_data` are returned as a tuple together with the deconvolution result.\n",
       "  * `features = setdiff(names(train), [y])` specifies which columns in `data` and `train` to be used as features - only applicable to the first form of this function.\n"
      ],
      "text/plain": [
       "\u001b[36m  dsea(data, train, y, train_predict[, bins_y, features]; kwargs...)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  dsea(X_data, X_train, y_train, train_predict[, bins_y]; kwargs...)\u001b[39m\n",
       "\n",
       "  Deconvolve the observed data with \u001b[4mDSEA/DSEA+\u001b[24m trained on the given training\n",
       "  set.\n",
       "\n",
       "  The data is provided as feature matrices \u001b[36mX_data\u001b[39m, \u001b[36mX_train\u001b[39m and the label\n",
       "  vector \u001b[36my_train\u001b[39m (or accordingly \u001b[36mdata[features]\u001b[39m, \u001b[36mtrain[features]\u001b[39m, and\n",
       "  \u001b[36mtrain[y]\u001b[39m). Here, \u001b[36my_train\u001b[39m must contain label indices rather than actual\n",
       "  values. All expected indices are optionally provided as \u001b[36mbins_y\u001b[39m.\n",
       "\n",
       "  The function object \u001b[36mtrain_predict(X_data, X_train, y_train, w_train) ->\n",
       "  Matrix\u001b[39m trains and applies a classifier to obtain a confidence matrix.\n",
       "\n",
       "  \u001b[1mKeyword arguments\u001b[22m\n",
       "\n",
       "    •    \u001b[36mf_0 = ones(m) ./ m\u001b[39m defines the prior, which is uniform by default\n",
       "\n",
       "    •    \u001b[36mfixweighting = true\u001b[39m sets, whether or not the weight update fix is\n",
       "        applied. This fix is proposed in my Master's thesis and in the\n",
       "        corresponding paper.\n",
       "\n",
       "    •    \u001b[36malpha = DEFAULT_STEPSIZE\u001b[39m is the step size taken in every\n",
       "        iteration.\n",
       "\n",
       "    •    \u001b[36msmoothing = Base.identity\u001b[39m is a function that optionally applies\n",
       "        smoothing in between iterations.\n",
       "\n",
       "    •    \u001b[36mK = 1\u001b[39m is the maximum number of iterations.\n",
       "\n",
       "    •    \u001b[36mepsilon = 0.0\u001b[39m is the minimum symmetric Chi Square distance between\n",
       "        iterations. If the actual distance is below this threshold,\n",
       "        convergence is assumed and the algorithm stops.\n",
       "\n",
       "    •    \u001b[36minspect = nothing\u001b[39m is a function \u001b[36m(f_k::Vector, k::Int,\n",
       "        chi2s::Float64, alphak::Float64) -> Any\u001b[39m optionally called in every\n",
       "        iteration.\n",
       "\n",
       "    •    \u001b[36mreturn_contributions = false\u001b[39m sets, whether or not the\n",
       "        contributions of individual examples in \u001b[36mX_data\u001b[39m are returned as a\n",
       "        tuple together with the deconvolution result.\n",
       "\n",
       "    •    \u001b[36mfeatures = setdiff(names(train), [y])\u001b[39m specifies which columns in\n",
       "        \u001b[36mdata\u001b[39m and \u001b[36mtrain\u001b[39m to be used as features - only applicable to the\n",
       "        first form of this function."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?dsea # You can find more information in the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "run(data, train, x, y[, bins_y]; kwargs...)\n",
       "\n",
       "run(x_data, x_train, y_train[, bins_y]; kwargs...)\n",
       "\n",
       "run(R, g; kwargs...)\n",
       "\\end{verbatim}\n",
       "Deconvolve the observed data applying the \\emph{Regularized Unfolding} trained on the given training set.\n",
       "\n",
       "The vectors \\texttt{x\\_data}, \\texttt{x\\_train}, and \\texttt{y\\_train} (or accordingly \\texttt{data[x]}, \\texttt{train[x]}, and \\texttt{train[y]}) must contain label/observation indices rather than actual values. All expected indices in \\texttt{y\\_train} are optionally provided as \\texttt{bins\\_y}. Alternatively, the detector response matrix \\texttt{R} and the observed density vector \\texttt{g} can be given directly.\n",
       "\n",
       "\\textbf{Keyword arguments}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{n\\_df = size(R, 2)} is the effective number of degrees of freedom. The default \\texttt{n\\_df} results in no regularization (there is one degree of freedom for each dimension in the result).\n",
       "\n",
       "\n",
       "\\item \\texttt{K = 100} is the maximum number of iterations.\n",
       "\n",
       "\n",
       "\\item \\texttt{epsilon = 1e-6} is the minimum difference in the loss function between iterations. RUN stops when the absolute loss difference drops below \\texttt{epsilon}.\n",
       "\n",
       "\n",
       "\\item \\texttt{acceptance\\_correction = nothing}  is a tuple of functions (ac(d), inv\\emph{ac(d)) representing the acceptance correction ac and its inverse operation inv}ac for a data set d.\n",
       "\n",
       "\n",
       "\\item \\texttt{ac\\_regularisation = true}  decides whether acceptance correction is taken into account for regularisation. Requires \\texttt{acceptance\\_correction} != nothing.\n",
       "\n",
       "\n",
       "\\item \\texttt{log\\_constant = 1/18394} is a selectable constant used in log regularisation to prevent the undefined case log(0).\n",
       "\n",
       "\n",
       "\\item \\texttt{inspect = nothing} is a function \\texttt{(f\\_k::Vector, k::Int, ldiff::Float64, tau::Float64) -> Any} optionally called in every iteration.\n",
       "\n",
       "\n",
       "\\item \\texttt{loggingstream = devnull} is an optional \\texttt{IO} stream to write log messages to.\n",
       "\n",
       "\n",
       "\\item \\texttt{fit\\_ratios = false} determines if ratios are fitted (i.e. \\texttt{R} has to contain counts so that the ratio \\texttt{f\\_est / f\\_train} is estimated) or if the probability density \\texttt{f\\_est} is fitted directly.\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Caution:} According to the value of \\texttt{fit\\_ratios}, the keyword argument \\texttt{f\\_0} specifies a ratio prior or a pdf prior, but only in the third form. In the other forms, \\texttt{f\\_0} always specifies a pdf prior.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "run(data, train, x, y[, bins_y]; kwargs...)\n",
       "\n",
       "run(x_data, x_train, y_train[, bins_y]; kwargs...)\n",
       "\n",
       "run(R, g; kwargs...)\n",
       "```\n",
       "\n",
       "Deconvolve the observed data applying the *Regularized Unfolding* trained on the given training set.\n",
       "\n",
       "The vectors `x_data`, `x_train`, and `y_train` (or accordingly `data[x]`, `train[x]`, and `train[y]`) must contain label/observation indices rather than actual values. All expected indices in `y_train` are optionally provided as `bins_y`. Alternatively, the detector response matrix `R` and the observed density vector `g` can be given directly.\n",
       "\n",
       "**Keyword arguments**\n",
       "\n",
       "  * `n_df = size(R, 2)` is the effective number of degrees of freedom. The default `n_df` results in no regularization (there is one degree of freedom for each dimension in the result).\n",
       "  * `K = 100` is the maximum number of iterations.\n",
       "  * `epsilon = 1e-6` is the minimum difference in the loss function between iterations. RUN stops when the absolute loss difference drops below `epsilon`.\n",
       "  * `acceptance_correction = nothing`  is a tuple of functions (ac(d), inv*ac(d)) representing the acceptance correction ac and its inverse operation inv*ac for a data set d.\n",
       "  * `ac_regularisation = true`  decides whether acceptance correction is taken into account for regularisation. Requires `acceptance_correction` != nothing.\n",
       "  * `log_constant = 1/18394` is a selectable constant used in log regularisation to prevent the undefined case log(0).\n",
       "  * `inspect = nothing` is a function `(f_k::Vector, k::Int, ldiff::Float64, tau::Float64) -> Any` optionally called in every iteration.\n",
       "  * `loggingstream = devnull` is an optional `IO` stream to write log messages to.\n",
       "  * `fit_ratios = false` determines if ratios are fitted (i.e. `R` has to contain counts so that the ratio `f_est / f_train` is estimated) or if the probability density `f_est` is fitted directly.\n",
       "\n",
       "**Caution:** According to the value of `fit_ratios`, the keyword argument `f_0` specifies a ratio prior or a pdf prior, but only in the third form. In the other forms, `f_0` always specifies a pdf prior.\n"
      ],
      "text/plain": [
       "\u001b[36m  run(data, train, x, y[, bins_y]; kwargs...)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  run(x_data, x_train, y_train[, bins_y]; kwargs...)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  run(R, g; kwargs...)\u001b[39m\n",
       "\n",
       "  Deconvolve the observed data applying the \u001b[4mRegularized Unfolding\u001b[24m trained on\n",
       "  the given training set.\n",
       "\n",
       "  The vectors \u001b[36mx_data\u001b[39m, \u001b[36mx_train\u001b[39m, and \u001b[36my_train\u001b[39m (or accordingly \u001b[36mdata[x]\u001b[39m, \u001b[36mtrain[x]\u001b[39m,\n",
       "  and \u001b[36mtrain[y]\u001b[39m) must contain label/observation indices rather than actual\n",
       "  values. All expected indices in \u001b[36my_train\u001b[39m are optionally provided as \u001b[36mbins_y\u001b[39m.\n",
       "  Alternatively, the detector response matrix \u001b[36mR\u001b[39m and the observed density\n",
       "  vector \u001b[36mg\u001b[39m can be given directly.\n",
       "\n",
       "  \u001b[1mKeyword arguments\u001b[22m\n",
       "\n",
       "    •    \u001b[36mn_df = size(R, 2)\u001b[39m is the effective number of degrees of freedom.\n",
       "        The default \u001b[36mn_df\u001b[39m results in no regularization (there is one degree\n",
       "        of freedom for each dimension in the result).\n",
       "\n",
       "    •    \u001b[36mK = 100\u001b[39m is the maximum number of iterations.\n",
       "\n",
       "    •    \u001b[36mepsilon = 1e-6\u001b[39m is the minimum difference in the loss function\n",
       "        between iterations. RUN stops when the absolute loss difference\n",
       "        drops below \u001b[36mepsilon\u001b[39m.\n",
       "\n",
       "    •    \u001b[36macceptance_correction = nothing\u001b[39m is a tuple of functions (ac(d),\n",
       "        inv\u001b[4mac(d)) representing the acceptance correction ac and its\n",
       "        inverse operation inv\u001b[24mac for a data set d.\n",
       "\n",
       "    •    \u001b[36mac_regularisation = true\u001b[39m decides whether acceptance correction is\n",
       "        taken into account for regularisation. Requires\n",
       "        \u001b[36macceptance_correction\u001b[39m != nothing.\n",
       "\n",
       "    •    \u001b[36mlog_constant = 1/18394\u001b[39m is a selectable constant used in log\n",
       "        regularisation to prevent the undefined case log(0).\n",
       "\n",
       "    •    \u001b[36minspect = nothing\u001b[39m is a function \u001b[36m(f_k::Vector, k::Int,\n",
       "        ldiff::Float64, tau::Float64) -> Any\u001b[39m optionally called in every\n",
       "        iteration.\n",
       "\n",
       "    •    \u001b[36mloggingstream = devnull\u001b[39m is an optional \u001b[36mIO\u001b[39m stream to write log\n",
       "        messages to.\n",
       "\n",
       "    •    \u001b[36mfit_ratios = false\u001b[39m determines if ratios are fitted (i.e. \u001b[36mR\u001b[39m has to\n",
       "        contain counts so that the ratio \u001b[36mf_est / f_train\u001b[39m is estimated) or\n",
       "        if the probability density \u001b[36mf_est\u001b[39m is fitted directly.\n",
       "\n",
       "  \u001b[1mCaution:\u001b[22m According to the value of \u001b[36mfit_ratios\u001b[39m, the keyword argument \u001b[36mf_0\u001b[39m\n",
       "  specifies a ratio prior or a pdf prior, but only in the third form. In the\n",
       "  other forms, \u001b[36mf_0\u001b[39m always specifies a pdf prior."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?CherenkovDeconvolution.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "ibu(data, train, x, y[, bins_y]; kwargs...)\n",
       "\n",
       "ibu(x_data, x_train, y_train[, bins_y]; kwargs...)\n",
       "\n",
       "ibu(R, g; kwargs...)\n",
       "\\end{verbatim}\n",
       "Deconvolve the observed data applying the \\emph{Iterative Bayesian Unfolding} trained on the given training set.\n",
       "\n",
       "The vectors \\texttt{x\\_data}, \\texttt{x\\_train}, and \\texttt{y\\_train} (or accordingly \\texttt{data[x]}, \\texttt{train[x]}, and \\texttt{train[y]}) must contain label/observation indices rather than actual values. All expected indices in \\texttt{y\\_train} are optionally provided as \\texttt{bins\\_y}. Alternatively, the detector response matrix \\texttt{R} and the observed density vector \\texttt{g} can be given directly.\n",
       "\n",
       "\\textbf{Keyword arguments}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{f\\_0 = ones(m) ./ m} defines the prior, which is uniform by default.\n",
       "\n",
       "\n",
       "\\item \\texttt{smoothing = Base.identity} is a function that optionally applies smoothing in between iterations. The operation is neither applied to the initial prior, nor to the final result. The function \\texttt{inspect} is called before the smoothing is performed.\n",
       "\n",
       "\n",
       "\\item \\texttt{K = 3} is the maximum number of iterations.\n",
       "\n",
       "\n",
       "\\item \\texttt{epsilon = 0.0} is the minimum symmetric Chi Square distance between iterations. If the actual distance is below this threshold, convergence is assumed and the algorithm stops.\n",
       "\n",
       "\n",
       "\\item \\texttt{alpha = DEFAULT\\_STEPSIZE} is the step size taken in every iteration.\n",
       "\n",
       "\n",
       "\\item \\texttt{fit\\_ratios = false} determines if ratios are fitted (i.e. \\texttt{R} has to contain counts so that the ratio \\texttt{f\\_est / f\\_train} is estimated) or if the probability density \\texttt{f\\_est} is fitted directly.\n",
       "\n",
       "\n",
       "\\item \\texttt{inspect = nothing} is a function \\texttt{(f\\_k::Vector, k::Int, chi2s::Float64, alphak::Float64) -> Any} optionally called in every iteration.\n",
       "\n",
       "\n",
       "\\item \\texttt{loggingstream = DevNull} is an optional \\texttt{IO} stream to write log messages to.\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Caution:} According to the value of \\texttt{fit\\_ratios}, the keyword argument \\texttt{f\\_0} specifies a ratio prior or a pdf prior, but only in the third form. In the other forms, \\texttt{f\\_0} always specifies a pdf prior.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "ibu(data, train, x, y[, bins_y]; kwargs...)\n",
       "\n",
       "ibu(x_data, x_train, y_train[, bins_y]; kwargs...)\n",
       "\n",
       "ibu(R, g; kwargs...)\n",
       "```\n",
       "\n",
       "Deconvolve the observed data applying the *Iterative Bayesian Unfolding* trained on the given training set.\n",
       "\n",
       "The vectors `x_data`, `x_train`, and `y_train` (or accordingly `data[x]`, `train[x]`, and `train[y]`) must contain label/observation indices rather than actual values. All expected indices in `y_train` are optionally provided as `bins_y`. Alternatively, the detector response matrix `R` and the observed density vector `g` can be given directly.\n",
       "\n",
       "**Keyword arguments**\n",
       "\n",
       "  * `f_0 = ones(m) ./ m` defines the prior, which is uniform by default.\n",
       "  * `smoothing = Base.identity` is a function that optionally applies smoothing in between iterations. The operation is neither applied to the initial prior, nor to the final result. The function `inspect` is called before the smoothing is performed.\n",
       "  * `K = 3` is the maximum number of iterations.\n",
       "  * `epsilon = 0.0` is the minimum symmetric Chi Square distance between iterations. If the actual distance is below this threshold, convergence is assumed and the algorithm stops.\n",
       "  * `alpha = DEFAULT_STEPSIZE` is the step size taken in every iteration.\n",
       "  * `fit_ratios = false` determines if ratios are fitted (i.e. `R` has to contain counts so that the ratio `f_est / f_train` is estimated) or if the probability density `f_est` is fitted directly.\n",
       "  * `inspect = nothing` is a function `(f_k::Vector, k::Int, chi2s::Float64, alphak::Float64) -> Any` optionally called in every iteration.\n",
       "  * `loggingstream = DevNull` is an optional `IO` stream to write log messages to.\n",
       "\n",
       "**Caution:** According to the value of `fit_ratios`, the keyword argument `f_0` specifies a ratio prior or a pdf prior, but only in the third form. In the other forms, `f_0` always specifies a pdf prior.\n"
      ],
      "text/plain": [
       "\u001b[36m  ibu(data, train, x, y[, bins_y]; kwargs...)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  ibu(x_data, x_train, y_train[, bins_y]; kwargs...)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  ibu(R, g; kwargs...)\u001b[39m\n",
       "\n",
       "  Deconvolve the observed data applying the \u001b[4mIterative Bayesian Unfolding\u001b[24m\n",
       "  trained on the given training set.\n",
       "\n",
       "  The vectors \u001b[36mx_data\u001b[39m, \u001b[36mx_train\u001b[39m, and \u001b[36my_train\u001b[39m (or accordingly \u001b[36mdata[x]\u001b[39m, \u001b[36mtrain[x]\u001b[39m,\n",
       "  and \u001b[36mtrain[y]\u001b[39m) must contain label/observation indices rather than actual\n",
       "  values. All expected indices in \u001b[36my_train\u001b[39m are optionally provided as \u001b[36mbins_y\u001b[39m.\n",
       "  Alternatively, the detector response matrix \u001b[36mR\u001b[39m and the observed density\n",
       "  vector \u001b[36mg\u001b[39m can be given directly.\n",
       "\n",
       "  \u001b[1mKeyword arguments\u001b[22m\n",
       "\n",
       "    •    \u001b[36mf_0 = ones(m) ./ m\u001b[39m defines the prior, which is uniform by default.\n",
       "\n",
       "    •    \u001b[36msmoothing = Base.identity\u001b[39m is a function that optionally applies\n",
       "        smoothing in between iterations. The operation is neither applied\n",
       "        to the initial prior, nor to the final result. The function\n",
       "        \u001b[36minspect\u001b[39m is called before the smoothing is performed.\n",
       "\n",
       "    •    \u001b[36mK = 3\u001b[39m is the maximum number of iterations.\n",
       "\n",
       "    •    \u001b[36mepsilon = 0.0\u001b[39m is the minimum symmetric Chi Square distance between\n",
       "        iterations. If the actual distance is below this threshold,\n",
       "        convergence is assumed and the algorithm stops.\n",
       "\n",
       "    •    \u001b[36malpha = DEFAULT_STEPSIZE\u001b[39m is the step size taken in every\n",
       "        iteration.\n",
       "\n",
       "    •    \u001b[36mfit_ratios = false\u001b[39m determines if ratios are fitted (i.e. \u001b[36mR\u001b[39m has to\n",
       "        contain counts so that the ratio \u001b[36mf_est / f_train\u001b[39m is estimated) or\n",
       "        if the probability density \u001b[36mf_est\u001b[39m is fitted directly.\n",
       "\n",
       "    •    \u001b[36minspect = nothing\u001b[39m is a function \u001b[36m(f_k::Vector, k::Int,\n",
       "        chi2s::Float64, alphak::Float64) -> Any\u001b[39m optionally called in every\n",
       "        iteration.\n",
       "\n",
       "    •    \u001b[36mloggingstream = DevNull\u001b[39m is an optional \u001b[36mIO\u001b[39m stream to write log\n",
       "        messages to.\n",
       "\n",
       "  \u001b[1mCaution:\u001b[22m According to the value of \u001b[36mfit_ratios\u001b[39m, the keyword argument \u001b[36mf_0\u001b[39m\n",
       "  specifies a ratio prior or a pdf prior, but only in the third form. In the\n",
       "  other forms, \u001b[36mf_0\u001b[39m always specifies a pdf prior."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?CherenkovDeconvolution.ibu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "svd(data, train, x, y[, bins_y]; kwargs...)\n",
       "\n",
       "svd(x_data, x_train, y_train[, bins_y]; kwargs...)\n",
       "\n",
       "svd(R, g; kwargs...)\n",
       "\\end{verbatim}\n",
       "Deconvolve the observed data applying the \\emph{SVD-based deconvolution algorithm} trained on the given training set.\n",
       "\n",
       "The vectors \\texttt{x\\_data}, \\texttt{x\\_train}, and \\texttt{y\\_train} (or accordingly \\texttt{data[x]}, \\texttt{train[x]}, and \\texttt{train[y]}) must contain label/observation indices rather than actual values. All expected indices in \\texttt{y\\_train} are optionally provided as \\texttt{bins\\_y}. Alternatively, the detector response matrix \\texttt{R} and the observed density vector \\texttt{g} can be given directly.\n",
       "\n",
       "\\textbf{Keyword arguments}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{effective\\_rank = -1} is a regularization parameter which defines the effective rank of the solution. This rank must be <= dim(f). Any value smaller than one results turns off regularization.\n",
       "\n",
       "\n",
       "\\item \\texttt{N = length(x\\_data)} is the number of observations. In the third form of the method, \\texttt{N=sum(g)} is the default, assuming that \\texttt{g} contains absolute counts, not probabilities.\n",
       "\n",
       "\n",
       "\\item \\texttt{B = DeconvUtil.cov\\_Poisson(g, N)} is the varianca-covariance matrix of the observed bins. The default value represents the assumption that each observed bin is Poisson-distributed with rate \\texttt{g[i]*N}.\n",
       "\n",
       "\n",
       "\\item \\texttt{epsilon\\_C = 1e-3} is a small constant to be added to each diagonal entry of the regularization matrix \\texttt{C}. If no such constant would be added, inversion of \\texttt{C} would not be possible.\n",
       "\n",
       "\n",
       "\\item \\texttt{fit\\_ratios = false} determines if ratios are fitted (i.e. \\texttt{R} has to contain counts so that the ratio \\texttt{f\\_est / f\\_train} is estimated) or if the probability density \\texttt{f\\_est} is fitted directly.\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Caution:} According to the value of \\texttt{fit\\_ratios}, the keyword argument \\texttt{f\\_0} specifies a ratio prior or a pdf prior, but only in the third form. In the other forms, \\texttt{f\\_0} always specifies a pdf prior.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "svd(data, train, x, y[, bins_y]; kwargs...)\n",
       "\n",
       "svd(x_data, x_train, y_train[, bins_y]; kwargs...)\n",
       "\n",
       "svd(R, g; kwargs...)\n",
       "```\n",
       "\n",
       "Deconvolve the observed data applying the *SVD-based deconvolution algorithm* trained on the given training set.\n",
       "\n",
       "The vectors `x_data`, `x_train`, and `y_train` (or accordingly `data[x]`, `train[x]`, and `train[y]`) must contain label/observation indices rather than actual values. All expected indices in `y_train` are optionally provided as `bins_y`. Alternatively, the detector response matrix `R` and the observed density vector `g` can be given directly.\n",
       "\n",
       "**Keyword arguments**\n",
       "\n",
       "  * `effective_rank = -1` is a regularization parameter which defines the effective rank of the solution. This rank must be <= dim(f). Any value smaller than one results turns off regularization.\n",
       "  * `N = length(x_data)` is the number of observations. In the third form of the method, `N=sum(g)` is the default, assuming that `g` contains absolute counts, not probabilities.\n",
       "  * `B = DeconvUtil.cov_Poisson(g, N)` is the varianca-covariance matrix of the observed bins. The default value represents the assumption that each observed bin is Poisson-distributed with rate `g[i]*N`.\n",
       "  * `epsilon_C = 1e-3` is a small constant to be added to each diagonal entry of the regularization matrix `C`. If no such constant would be added, inversion of `C` would not be possible.\n",
       "  * `fit_ratios = false` determines if ratios are fitted (i.e. `R` has to contain counts so that the ratio `f_est / f_train` is estimated) or if the probability density `f_est` is fitted directly.\n",
       "\n",
       "**Caution:** According to the value of `fit_ratios`, the keyword argument `f_0` specifies a ratio prior or a pdf prior, but only in the third form. In the other forms, `f_0` always specifies a pdf prior.\n"
      ],
      "text/plain": [
       "\u001b[36m  svd(data, train, x, y[, bins_y]; kwargs...)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  svd(x_data, x_train, y_train[, bins_y]; kwargs...)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  svd(R, g; kwargs...)\u001b[39m\n",
       "\n",
       "  Deconvolve the observed data applying the \u001b[4mSVD-based deconvolution algorithm\u001b[24m\n",
       "  trained on the given training set.\n",
       "\n",
       "  The vectors \u001b[36mx_data\u001b[39m, \u001b[36mx_train\u001b[39m, and \u001b[36my_train\u001b[39m (or accordingly \u001b[36mdata[x]\u001b[39m, \u001b[36mtrain[x]\u001b[39m,\n",
       "  and \u001b[36mtrain[y]\u001b[39m) must contain label/observation indices rather than actual\n",
       "  values. All expected indices in \u001b[36my_train\u001b[39m are optionally provided as \u001b[36mbins_y\u001b[39m.\n",
       "  Alternatively, the detector response matrix \u001b[36mR\u001b[39m and the observed density\n",
       "  vector \u001b[36mg\u001b[39m can be given directly.\n",
       "\n",
       "  \u001b[1mKeyword arguments\u001b[22m\n",
       "\n",
       "    •    \u001b[36meffective_rank = -1\u001b[39m is a regularization parameter which defines\n",
       "        the effective rank of the solution. This rank must be <= dim(f).\n",
       "        Any value smaller than one results turns off regularization.\n",
       "\n",
       "    •    \u001b[36mN = length(x_data)\u001b[39m is the number of observations. In the third\n",
       "        form of the method, \u001b[36mN=sum(g)\u001b[39m is the default, assuming that \u001b[36mg\u001b[39m\n",
       "        contains absolute counts, not probabilities.\n",
       "\n",
       "    •    \u001b[36mB = DeconvUtil.cov_Poisson(g, N)\u001b[39m is the varianca-covariance matrix\n",
       "        of the observed bins. The default value represents the assumption\n",
       "        that each observed bin is Poisson-distributed with rate \u001b[36mg[i]*N\u001b[39m.\n",
       "\n",
       "    •    \u001b[36mepsilon_C = 1e-3\u001b[39m is a small constant to be added to each diagonal\n",
       "        entry of the regularization matrix \u001b[36mC\u001b[39m. If no such constant would be\n",
       "        added, inversion of \u001b[36mC\u001b[39m would not be possible.\n",
       "\n",
       "    •    \u001b[36mfit_ratios = false\u001b[39m determines if ratios are fitted (i.e. \u001b[36mR\u001b[39m has to\n",
       "        contain counts so that the ratio \u001b[36mf_est / f_train\u001b[39m is estimated) or\n",
       "        if the probability density \u001b[36mf_est\u001b[39m is fitted directly.\n",
       "\n",
       "  \u001b[1mCaution:\u001b[22m According to the value of \u001b[36mfit_ratios\u001b[39m, the keyword argument \u001b[36mf_0\u001b[39m\n",
       "  specifies a ratio prior or a pdf prior, but only in the third form. In the\n",
       "  other forms, \u001b[36mf_0\u001b[39m always specifies a pdf prior."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?CherenkovDeconvolution.svd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
